{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP49izA8waK9TUwWUaPF1jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooeun921/Big-Data-Analyst/blob/main/Part02_Section_03_data_preprocessing_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucVNULyQCgts"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 03 í•™ìŠµ : scikit-learnì„ í™œìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "gHEjCHK8CxvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/dat.csv')\n",
        "\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "x9fTnv7ADETD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "h2XGsUYpWdk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XëŠ” gradeë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì—´. yëŠ” grade => ì¢…ì†ë³€ìˆ˜ì´ì, ë°˜ì‘ë³€ìˆ˜\n",
        "\n",
        "X = df.drop(['grade'], axis=1)\n",
        "y  = df.grade"
      ],
      "metadata": {
        "id": "KOegBgzkC9Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë°ì´í„° ë¶„í• "
      ],
      "metadata": {
        "id": "6atUfu7faBEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle = True, stratify = None)"
      ],
      "metadata": {
        "id": "ZqKFdhaFWwzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™êµ ì¢…ë¥˜ê°€ GP, MS ìœ¼ë¡œ ë‚˜ëˆ ì ¸ ìˆìœ¼ë¯€ë¡œ, stratify(ì¸µí™”ì¶”ì¶œ)ì„ í™œìš©í•˜ì—¬ ê° í•™êµ ì¢…ë¥˜ì— ë”°ë¥¸ ë™ì¼í•œ ë¹„ìœ¨ë¡œ train, test ë°ì´í„° ë¶„í• . ë¹„ìŠ·í•œ ë§¥ë½ìœ¼ë¡œ, ì„±ë³„ë¡œë„ ê°€ëŠ¥.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify = X['school'])"
      ],
      "metadata": {
        "id": "NqRIxdacXS2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
        "\n",
        "y_train.hist(ax = axs[0], color = 'blue', alpha = 0.7)\n",
        "axs[0].set_title('histogram of train y')\n",
        "\n",
        "y_test.hist(ax = axs[1], color = 'red', alpha = 0.7)\n",
        "axs[1].set_title('histogram of test y')\n",
        "\n",
        "plt.tight_layout();\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T-WAKNdmX-dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "ëŒ€ì¹˜ë²• ì¢…ë¥˜ì—ëŠ” ëŒ€í‘œê°’(í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’)ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ê³¼ KNNì„ ì´ìš©í•œ ëŒ€ì¹˜ë²•ì´ ìˆìŒ.\n",
        "\n",
        "ëŒ€ì¹˜ë²•ì„ í™œìš©í•  ë•Œ, ê²°ì¸¡ì¹˜ë¥¼ ëŒ€ì¹˜í•  ê°’ì„ ì„ ì •í•œ ë’¤ì— ì‹¤ì œë¡œ êµì²´í•˜ëŠ” ê³¼ì •ì—ì„œ, `fit()`ê³¼ `transform()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.\n",
        "\n",
        "`fit()` : ë°ì´í„°(í‰ê· , ì¤‘ì•™ê°’ ë“±)ë¥¼ í•™ìŠµ.\n",
        "`transform()` : í•™ìŠµëœ ë°ì´í„°ë¥¼ í™œìš©í•´ì„œ ë°ì´í„°ë¥¼ ë³€í™˜.\n",
        "\n",
        "`fit_transform()` : í•œë²ˆì— ì ìš©í•  ìˆ˜ ìˆìŒ.\n",
        "\n",
        "ğŸ ë‹¨, ì´ëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œë§Œ í™œìš©ë˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” `transform()`ë§Œ ì‚¬ìš©í•˜ëŠ”ë°, í•™ìŠµ ì‹œì— í™œìš©í•œ ê¸°ì¤€ìœ¼ë¡œë§Œ ëŒ€ì¹˜í•´ì•¼ í•˜ê³ . í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµì‹œí‚¨ ê±¸ í™œìš©í•˜ë©´ ë°ì´í„° ëˆ„ì¶œì´ ìƒê¸´ë‹¤ê³  ë³´ê¸° ë•Œë¬¸!\n",
        "\n",
        "\n",
        "```\n",
        "# í•™ìŠµì‹œí‚¬ ë°ì´í„° = imputer\n",
        "# ë³€í™˜í•  ë°ì´í„° = X_train or X_test\n",
        "\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "```"
      ],
      "metadata": {
        "id": "Pgo1nr95aEA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().sum())"
      ],
      "metadata": {
        "id": "-5r1WeR6YLpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê·  ëŒ€ì¹˜ë²•\n",
        "# ì‰½ê³  ë¹ ë¥´ê²Œ ëŒ€ì¹˜í•  ìˆ˜ ìˆì§€ë§Œ. ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•˜ê³ , ê²°ì¸¡ì¹˜ê°€ ë§ìœ¼ë©´ í‰ê· ê°’ì˜ ë¹ˆë„ìˆ˜ê°€ ë§ì•„ì§€ë¯€ë¡œ ë¶„í¬ê°€ ì™œê³¡ëœë‹¤.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "train_X1 = X_train.copy()\n",
        "test_X1 = X_test.copy()\n",
        "imputer_mean = SimpleImputer(strategy = 'mean')\n",
        "\n",
        "train_X1['goout'] = imputer_mean.fit_transform(train_X1[['goout']])\n",
        "test_X1['goout'] = imputer_mean.transform(test_X1[['goout']])\n",
        "\n",
        "print(test_X1['goout'].isna().sum())\n",
        "print(test_X1['goout'].isna().sum())"
      ],
      "metadata": {
        "id": "IGiXZGPgZ4w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¤‘ì•™ê°’ ëŒ€ì¹˜ë²•\n",
        "# ì‰½ê³  ë¹ ë¥´ê²Œ ëŒ€ì¹˜í•  ìˆ˜ ìˆì§€ë§Œ. ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•¨. ê²°ì¸¡ì¹˜ê°€ ë§ì„ ë•Œ ì¤‘ì•™ê°’ì˜ ë¹ˆë„ìˆ˜ê°€ ë§ì•„ì§€ë¯€ë¡œ ë¶„í¬ê°€ ì™œê³¡.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "train_X2 = X_train.copy()\n",
        "test_X2 = X_test.copy()\n",
        "imputer_median = SimpleImputer(strategy = 'median')\n",
        "\n",
        "train_X2['goout'] = imputer_median.fit_transform(train_X2[['goout']])\n",
        "test_X2['goout'] = imputer_median.transform(test_X2[['goout']])\n",
        "\n",
        "print(train_X2['goout'].isna().sum())\n",
        "print(test_X2['goout'].isna().sum())"
      ],
      "metadata": {
        "id": "DsbJc3l5et2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìµœë¹ˆê°’ ëŒ€ì¹˜ë²•\n",
        "# ì‰½ê³  ë¹ ë¥´ê²Œ ëŒ€ì¹˜í•  ìˆ˜ ìˆì§€ë§Œ. ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•¨. ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ì— í™œìš©í•  ê²½ìš° ë¶„í¬ ë¶ˆê· í˜•ì„ ì‹¬í™”ì‹œí‚¬ ìˆ˜ ìˆìŒ.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "train_X3 = X_train.copy()\n",
        "test_X3 = X_test.copy()\n",
        "imputer_frequent = SimpleImputer(strategy = 'most_frequent')\n",
        "\n",
        "train_X3['goout'] = imputer_frequent.fit_transform(train_X3[['goout']])\n",
        "test_X3['goout'] = imputer_frequent.transform(test_X3[['goout']])\n",
        "\n",
        "print(train_X3['goout'].isna().sum())\n",
        "print(test_X3['goout'].isna().sum())"
      ],
      "metadata": {
        "id": "gStF2Bxja0p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN(K-Nearest Neighbors)ì„ ì´ìš©í•œ ëŒ€ì¹˜ë²•\n",
        "# ë°ì´í„°ì— ëŒ€í•œ ê°€ì • ì—†ì´ ì‰½ê³  ë¹ ë¥´ê²Œ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ ê°€ëŠ¥\n",
        "# ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ë° ì´ìƒì¹˜ì— ë¯¼ê°í•˜ë©°, ê³ ì°¨ì› ë°ì´í„°ì˜ ê²½ìš° ëª¨ë¸ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "train_X4 = X_train.copy()\n",
        "test_X4 = X_test.copy()\n",
        "\n",
        "# KNNì€ ê²°ì¸¡ì¹˜ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì´ì›ƒ ê°„ì˜ ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì—, ë²”ì£¼í˜• ë°ì´í„°ì—ëŠ” í™œìš©í•  ìˆ˜ ì—†ìŒ. -> ìˆ«ìí˜•ë§Œ ë”°ë¡œ ë½‘ì€ ë’¤ì— ëŒ€ì¹˜í•´ì¤˜ì•¼ í•¨.\n",
        "# ì—¬ê¸°ì„œë¶€í„° train_X4_numê³¼ test_X4_numì€ numpy.ndarray íƒ€ì…ìœ¼ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—,\n",
        "train_X4_num = train_X4.select_dtypes('number')\n",
        "test_X4_num = test_X4.select_dtypes('number')\n",
        "\n",
        "train_X4_cat = train_X4.select_dtypes('object')\n",
        "test_X4_cat = test_X4.select_dtypes('object')\n",
        "\n",
        "knnimputer = KNNImputer(n_neighbors  = 3)\n",
        "train_X4_num_imputed = knnimputer.fit_transform(train_X4_num)\n",
        "test_X4_num_imputed = knnimputer.transform(test_X4_num)\n",
        "\n",
        "train_X4_num_imputed = pd.DataFrame(train_X4_num_imputed, columns = train_X4_num.columns, index = train_X4.index)\n",
        "test_X4_num_imputed = pd.DataFrame(test_X4_num_imputed, columns = test_X4_num.columns, index = test_X4.index)\n",
        "\n",
        "# axis = 1 -> columns ê¸°ì¤€ìœ¼ë¡œ, ìˆ˜ì¹˜í˜• ë°ì´í„°(ëŒ€ì¹˜í•¨)ì™€ ë²”ì£¼í˜• ë°ì´í„° ë³‘í•©.\n",
        "train_X4 = pd.concat([train_X4_cat, train_X4_num_imputed], axis = 1)\n",
        "test_X4 = pd.concat([test_X4_cat, test_X4_num_imputed], axis = 1)\n",
        "\n",
        "print(train_X4['goout'].isna().sum())\n",
        "print(test_X4['goout'].isna().sum())"
      ],
      "metadata": {
        "id": "STM0Ju-zfK-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë³´í†µì˜ Imputer(SimpleImputer, KNNImputer, StandardScaler ë“±)ì€ ì‚¬ìš© ì‹œ ë°ì´í„°ë¥¼ numpy.ndarray í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤.\n",
        "ì´ë•Œ, Imputerë¥¼ ì •ì˜í•  ë•Œ `.set_output(transform = 'pandas')` ë¥¼ ë¶™ì´ë©´ ìë™ìœ¼ë¡œ DataFrame íƒ€ì…ìœ¼ë¡œ ë°˜í™˜í•´ì¤Œ."
      ],
      "metadata": {
        "id": "ESdB8Peuov6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knnimputer2 = KNNImputer(n_neighbors = 5).set_output(transform = 'pandas')\n",
        "train_X4_num_imputed2 = knnimputer2.fit_transform(train_X4_num)\n",
        "test_X4_num_imputed2 = knnimputer2.transform(test_X4_num)\n",
        "\n",
        "print(train_X4_num_imputed2.head())"
      ],
      "metadata": {
        "id": "5q6KHvxkifdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandasë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬í•˜ê¸°\n",
        "\n",
        "data = {\n",
        "    'í•™ìƒ': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ìˆ˜ì§€', 'ì§€í˜„'],\n",
        "    'ìˆ˜í•™': [85, np.nan, 78, np.nan, 93],\n",
        "    'ì˜ì–´': [np.nan, 88, 79, 85, np.nan],\n",
        "    'ê³¼í•™': [92, 85, np.nan, 80, 88]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "aAH6HuE_ndl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
        "# inplaceëŠ” ê¸°ì¡´ì˜ ë°ì´í„°í”„ë ˆì„ì—ì„œ ê°’ì„ ë³€ê²½í• ì§€, ì•„ë‹ˆë©´ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ì§€ì— ê´€í•œ íŒŒë¼ë¯¸í„°ì„.\n",
        "# Falseë¥¼ ì£¼ëŠ” ê²½ìš°, ìƒˆë¡œìš´ ë³€ìˆ˜ì— í• ë‹¹ì„ í•´ì¤˜ì•¼, ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë³€í™”ê°€ ì—†ê³  ìƒˆ ë°ì´í„°í”„ë ˆì„ì—ë§Œ ê²°ì¸¡ì¹˜ê°€ ì±„ì›Œì§„ ê²°ê³¼ê°€ ë‚˜ì˜´.\n",
        "\n",
        "df1 = df.copy()\n",
        "\n",
        "# ê·¼ë° pythonì—ì„œ inplaceë¥¼ ì‚¬ìš©í•œ ë°©ì‹ì´ ì ì  ì¤„ì–´ë“¤ë‹¤ê°€ ì‚¬ë¼ì§€ëŠ” ì¶”ì„¸ë¼, ì›ë³¸ì„ ìˆ˜ì •í•˜ë ¤ë©´ ì§ì ‘ í• ë‹¹í•˜ëŠ” ê²ƒì´ ë‚«ë‹¤ê³  ê²½ê³ ë¬¸ ëœ¸.\n",
        "# df1['ìˆ˜í•™'].fillna(df1['ìˆ˜í•™'].mean(), inplace = True)\n",
        "# df1['ì˜ì–´'].fillna(50, inplace = True)\n",
        "# df1['ê³¼í•™'].fillna(method = 'ffill',  inplace = True)\n",
        "\n",
        "df1['ìˆ˜í•™'] = df1['ìˆ˜í•™'].fillna(df1['ìˆ˜í•™'].mean())\n",
        "df1['ì˜ì–´'] = df1['ì˜ì–´'].fillna(50)\n",
        "df1['ê³¼í•™'] = df1['ê³¼í•™'].ffill()\n",
        "\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "2T0dROZQp8-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.copy()\n",
        "\n",
        "# apply í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œì„ ë”°ë¡œ ë½‘ìŒ. -> df2_numì´ë¼ëŠ” ìƒˆë¡œìš´ DataFrameì— ì €ì¥.\n",
        "df2_num = df2.select_dtypes('number')\n",
        "\n",
        "# apply í•¨ìˆ˜ì™€ lambdaë¥¼ ì‚¬ìš©í•˜ì—¬ np.nan ê²°ì¸¡ì¹˜ë¥¼ ê·¸ ì¹¼ëŸ¼(ì—´)ì˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€. df2_numì€ ê²°ì¸¡ì¹˜ê°€ ì „ë¶€ ì±„ì›Œì ¸ìˆê³ , ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì¡´ì¬í•¨.\n",
        "df2_num = df2_num.apply(lambda col : col.fillna(col.mean()))\n",
        "\n",
        "# ê¸°ì¡´ì˜ df2 DataFrameì—ì„œ ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ ë“  ì—´ë§Œ ë½‘ì•„ì„œ, df2_num(ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ”)ì˜ ê°’ìœ¼ë¡œ ë³€ê²½.\n",
        "df2[df2_num.columns] = df2_num\n",
        "\n",
        "print(df2)"
      ],
      "metadata": {
        "id": "WRj5R6sAqJBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬\n",
        "ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ. ëŒ€í‘œì ì¸ ë³€ìˆ˜ ì¸ì½”ë”© ë°©ì‹ìœ¼ë¡œëŠ”, Ladel encoding, One-hot encoding, Dummy encoding ë“±ì´ ìˆë‹¤.\n",
        "\n",
        "ğŸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¶•ì†Œ(Class coercing)\n",
        "\n",
        "ê·¸ëŸ¬ë‚˜, ë²”ì£¼í˜• ì„¤ëª…ë³€ìˆ˜(ex. ì§€ì—­) ê°™ì€ ê²½ìš° ë²”ì£¼ì˜ ìˆ˜ê°€ ë§ê±°ë‚˜ ë¶ˆê· í˜•ì¸ ê²½ìš°ì— ì›-í•« ì¸ì½”ë”©ì„ í•˜ëŠ” ê²½ìš° ì¹¼ëŸ¼ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ ëª¨ë¸ í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ. -> ë²”ì£¼ ê°„ì˜ í†µí•©ì´ í•„ìš”í•œ ì´ìœ .\n"
      ],
      "metadata": {
        "id": "wXkKyFNSukFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding\n",
        "# ë²”ì£¼í˜• ë³€ìˆ˜ì˜ labelì— ì•ŒíŒŒë²³ ìˆœì„œëŒ€ë¡œ ê³ ìœ í•œ ì •ìˆ˜ë¥¼ í• ë‹¹.\n",
        "# ìˆœì„œí˜• ë³€ìˆ˜ì˜ ê²½ìš°, ìˆœì„œë¥¼ ë°˜ì˜í•œ ì¸ì½”ë”© ê°€ëŠ¥.\n",
        "# ìˆœì„œê°€ ì—†ëŠ” ë³€ìˆ˜ì˜ ê²½ìš°, ì˜ëª»ëœ ìˆœì„œ ì •ë³´ê°€ ë°˜ì˜ë˜ëŠ” ë¬¸ì œ ë°œìƒ.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "\n",
        "train_X5 = X_train.copy()\n",
        "test_X5 = X_test.copy()\n",
        "\n",
        "train_X5_cat = train_X5.select_dtypes('object')\n",
        "test_X5_cat = test_X5.select_dtypes('object')"
      ],
      "metadata": {
        "id": "tc6niCOguqnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LabelEncoderëŠ” 1ì°¨ì›. í•˜ë‚˜ì˜ ì—´ì— ëŒ€í•´ì„œë§Œ ì¸ì½”ë”© ê°€ëŠ¥í•˜ê³ ,\n",
        "# OrdinalEncoder ê°™ì€ ê²½ìš°ì—ëŠ” 2ì°¨ì›. ì—¬ëŸ¬ì—´ì„ ë™ì‹œì— ì¸ì½”ë”©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë³´í†µ OrdinalEncoderë¥¼ ì‚¬ìš©í•¨.\n",
        "\n",
        "# labelencoder = LabelEncoder().set_output(transform = 'pandas')\n",
        "ordinalencoder = OrdinalEncoder().set_output(transform = 'pandas')\n",
        "train_X5_cat = ordinalencoder.fit_transform(train_X5_cat)\n",
        "test_X5_cat = ordinalencoder.transform(test_X5_cat)\n",
        "\n",
        "print(train_X5_cat.head())"
      ],
      "metadata": {
        "id": "-6iVGDxoxUaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test ì¸ì½”ë”©ì„ í•  ë•Œ, test dataëŠ” trainì—ì„œ í•™ìŠµí•œ ë²”ì£¼ì— ëŒ€í•´ì„œë§Œ ë²”ì£¼ ë³€ê²½ì„ í•˜ë¯€ë¡œ,\n",
        "# train_dataì—ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°’ì´ test dataì—ì„œ ì£¼ì–´ì§€ê²Œ ë˜ë©´ ValueErrorê°€ ë°œìƒí•¨.\n",
        "# ë”°ë¼ì„œ ì´ë¥¼ ë§‰ê¸° ìœ„í•´, OrdinalEncoderë¥¼ ì„¤ì •í•  ë•Œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ í•´ì¤˜ì•¼ í•œë‹¤.\n",
        "\n",
        "train_data = pd.DataFrame( { 'job' : ['Doctor', 'Engineer', 'Teacher', 'Nurse'] } )\n",
        "test_data = pd.DataFrame( { 'job' : ['Doctor', 'Lawyer', 'Teacher', 'Scientist'] } )\n",
        "\n",
        "# handle_unknown : {'error', 'use_encoded_value'}, default='error'\n",
        "'''\n",
        "When set to\n",
        "        'use_encoded_value', the encoded value of unknown categories will be\n",
        "        set to the value given for the parameter `unknown_value`.\n",
        "'''\n",
        "\n",
        "oe = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\n",
        "\n",
        "train_data['job_encoded'] = oe.fit_transform(train_data[['job']])\n",
        "test_data['job_encoded'] = oe.transform(test_data[['job']])\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "TjzlfNLhxiEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "# Label encodingì˜ ë²”ì£¼í˜• ë³€ìˆ˜ì— ìˆœì„œ ì •ë³´ê°€ ë°˜ì˜ë˜ëŠ” ë¬¸ì œ í•´ê²° ê°€ëŠ¥\n",
        "# ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ë§ì„ ê²½ìš°, ì°¨ì›ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ê³„ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚˜ëŠ” ë¬¸ì œ ë°œìƒ.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "train_X6 = X_train.copy()\n",
        "test_X6 = X_test.copy()\n",
        "\n",
        "train_X6_cat = train_X6.select_dtypes('object')\n",
        "test_X6_cat = test_X6.select_dtypes('object')"
      ],
      "metadata": {
        "id": "L4eEpD7906RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse_outputëŠ” í¬ì†Œí–‰ë ¬ / ì¼ë°˜ì ì¸ ë°€ì§‘í–‰ë ¬ ì„ íƒ ì˜µì…˜ì„. ì‚¬ëŒì´ ë³´ê¸° í¸í•œ ìª½ì€ ë°€ì§‘í–‰ë ¬ì´ê¸° ë•Œë¬¸ì— ë”°ë¡œ ì‹œê°í™”ìš©ë„ë©´ Falseê°€ ë§ì§€ë§Œ,\n",
        "# ê·¸ëƒ¥ ë°ì´í„° ì „ì²˜ë¦¬ í›„ì— ëª¨ë¸ ëŒë¦¬ëŠ” ëª©ì ìœ¼ë¡œ ì¸ì½”ë”©ì„ í•˜ëŠ” ê±°ë¼ë©´ êµ³ì´, sparse_output ì„ Falseë¡œ í•  í•„ìš”ê°€ ì—†ìŒ.\n",
        "onehotencoder = OneHotEncoder(sparse_output = False, handle_unknown  = 'ignore').set_output(transform = 'pandas')\n",
        "\n",
        "train_X6_cat = onehotencoder.fit_transform(train_X6_cat)\n",
        "test_X6_cat = onehotencoder.transform(test_X6_cat)\n",
        "\n",
        "print(train_X6_cat.head())"
      ],
      "metadata": {
        "id": "tiT4qBRR4EuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy encoding\n",
        "# ë²”ì£¼í˜• ë³€ ìˆ˜ì˜ ê° ë²”ì£¼ ìˆ˜ -1ê°œ ë§Œí¼ì˜ ìƒˆë¡œìš´ ì—´ì„ ìƒŒì„±í•˜ê³ , 1 ë˜ëŠ” 0ì˜ ê°’ì„ ë¶€ì—¬í•´ì„œ ê° ë²”ì£¼ë¥¼ êµ¬ë¶„í•˜ëŠ” ë°©ë²•.\n",
        "# ê¸°ì¡´ì˜ ì›-í•«ì¸ì½”ë”©ì—ì„œ ì¹¼ëŸ¼ 1ê°œë¥¼ ì œê±°í•˜ë©´ ë”ë¯¸ ì¸ì½”ë”©ì´ ë¨.\n",
        "# ì¥ì  :  Label encodingì˜ ë¬¸ì œì ì¸ ìˆœì„œ ì •ë³´ê°€ ë°˜ì˜ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²° ê°€ëŠ¥\n",
        "# ë‹¨ì  : One-hot encodingê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ë§ì„ ê²½ìš° ì°¨ì›ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ê³„ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚¨.\n",
        "\n",
        "train_X7 = X_train.copy()\n",
        "test_X7 = X_test.copy()\n",
        "\n",
        "train_X7_cat = train_X7.select_dtypes('object')\n",
        "test_X7_cat = test_X7.select_dtypes('object')\n",
        "\n",
        "oe = OneHotEncoder(drop = 'first', sparse_output = False, handle_unknown='error').set_output(transform = 'pandas')\n",
        "\n",
        "train_X7_cat = oe.fit_transform(train_X7_cat)\n",
        "test_X7_cat = oe.transform(test_X7_cat)\n",
        "\n",
        "print(train_X7_cat.head())"
      ],
      "metadata": {
        "id": "6CPQBoes3mk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ì¡´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë”°ë¡œ ë³´ì¡´\n",
        "train_X7_num = train_X7.select_dtypes(exclude='object')\n",
        "test_X7_num = test_X7.select_dtypes(exclude='object')\n",
        "\n",
        "# ìˆ˜ì¹˜í˜• + ì¸ì½”ë”©ëœ ë²”ì£¼í˜• ë³‘í•©\n",
        "train_X7 = pd.concat([train_X7_num, train_X7_cat], axis=1)\n",
        "test_X7 = pd.concat([test_X7_num, test_X7_cat], axis=1)\n",
        "\n",
        "print(train_X7.head())"
      ],
      "metadata": {
        "id": "5WvKDtVaDtb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë²”ì£¼í˜• ë³€ìˆ˜ ë²”ì£¼ ì¶•ì†Œ ë°©ë²•\n",
        "\n",
        "train_bike = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/bike_train.csv')\n",
        "test_bike = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/bike_test.csv')\n",
        "print(train_bike.head(2))"
      ],
      "metadata": {
        "id": "yud8dshcC3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_bike['weather'].value_counts())"
      ],
      "metadata": {
        "id": "TNrMB22eFYIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê° ë²”ì£¼ ê°„ì˜ ìƒëŒ€ì  ë¹ˆë„ë¡œ ê³„ì‚°.\n",
        "freq = train_bike['weather'].value_counts(normalize = True)\n",
        "print(freq)"
      ],
      "metadata": {
        "id": "84nqXBWBGE_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—¬ê¸°ì—ì„œ weather = 3, 4ëŠ” ë°ì´í„°ì ìœ¼ë¡œ í° ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì—, ë²”ì£¼ë¥¼ í†µí•©í•˜ëŠ” ê²Œ ë‚˜ìŒ.\n",
        "# í•´ë‹¹\n",
        "\n",
        "rare_categories = freq[freq < 0.1].index\n",
        "print(rare_categories)"
      ],
      "metadata": {
        "id": "2lzKqd50GbH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bike['weather'] = train_bike['weather'].mask(train_bike['weather'].isin(rare_categories), 'other')\n",
        "test_bike['weather'] = test_bike['weather'].mask(test_bike['weather'].isin(rare_categories), 'other')\n",
        "\n",
        "print(train_bike['weather'].value_counts())"
      ],
      "metadata": {
        "id": "VeeOzzwbHf6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë³€ìˆ˜ ë³€í™˜ ë° ìŠ¤ì¼€ì¼ë§\n",
        "ë°ì´í„° ë¶„í¬ì˜ ì¹˜ìš°ì¹¨ì´ ìˆì„ ë•Œ, ë³€ìˆ˜ ë³€í™˜ì„ í†µí•´ ì •ê·œë¶„í¬ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ.\n",
        "- ì¼ë°˜ ë³€í™˜ : Box-Cox ë³€í™˜, Yeo-Johnson ë³€í™˜, ë¡œê·¸ ë³€í™˜, ë£¨íŠ¸ ë³€í™˜.\n",
        "- ì •ê·œí™” ë°©ë²• : í‘œì¤€í™”(Standardization), min-max ë“±"
      ],
      "metadata": {
        "id": "02FMgNJ4Jp0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.warnings = warnings\n",
        "\n",
        "bike_data = pd.read_csv(\"https://raw.githubusercontent.com/YoungjinBD/data/main/bike_train.csv\")\n",
        "bike_data['count'].hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rM3prE32KTBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box_tr = PowerTransformer(method = 'box-cox') # method = 'yeo-johnson'ì´ default ê°’.\n",
        "bike_data['count_boxcox'] = box_tr.fit_transform(bike_data[['count']])\n",
        "\n",
        "print(\"lambda: \", box_tr.lambdas_)"
      ],
      "metadata": {
        "id": "u2MG129aLmxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data['count_log'] = np.log1p(bike_data[['count']])\n",
        "bike_data['count_sqrt'] = np.sqrt(bike_data[['count']])\n",
        "\n",
        "bike_data[['count', 'count_boxcox', 'count_log', 'count_sqrt']].hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cZ1Cx8ucMBR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì •ê·œí™” ë°©ë²•\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_X8 = X_train.copy()\n",
        "test_X8 = X_test.copy()\n",
        "\n",
        "standardscaler = StandardScaler().set_output(transform = 'pandas')\n",
        "\n",
        "train_X8_num = train_X8.select_dtypes('number')\n",
        "test_X8_num = test_X8.select_dtypes('number')\n",
        "\n",
        "train_X8_num = standardscaler.fit_transform(train_X8_num)\n",
        "test_X8_num = standardscaler.transform(test_X8_num)\n",
        "\n",
        "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
        "train_X8['absences'].hist(ax = axs[0], color = 'blue', alpha = 0.7)\n",
        "train_X8_num['absences'].hist(ax = axs[1], color = 'red', alpha = 0.7)\n",
        "axs[0].set_title('before')\n",
        "axs[1].set_title('after')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WRbjeGLPMjP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.round(train_X8['absences'].mean()))\n",
        "print(np.round(train_X8_num['absences'].mean()))\n",
        "print(np.round(train_X8['absences'].std(), 2))\n",
        "print(np.round(train_X8_num['absences'].std(), 2))"
      ],
      "metadata": {
        "id": "c2LUlw0XNfr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í–‰ : axis = 0\n",
        "\n",
        "ì—´ : axis = 1\n",
        "\n",
        "ì´ê²Œ ê¸°ë³¸ ë² ì´ìŠ¤ê¸´ í•˜ì§€ë§Œ, ì‹¤ì œ ê³„ì‚°í•  ë•ŒëŠ” axis = 0ì¼ ë•Œ ì—´ ê¸°ì¤€ì˜ í•©ì„ êµ¬í•˜ê²Œ ë¨. ì¦‰, columns ë³„ë¡œ í•©ì„ êµ¬í•  ë•ŒëŠ” axis = 0ì„."
      ],
      "metadata": {
        "id": "TsxbVvrwRagf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_X9 = X_train.copy()\n",
        "test_X9 = X_test.copy()\n",
        "\n",
        "train_X9_num = train_X9.select_dtypes('number')\n",
        "test_X9_num = test_X9.select_dtypes('number')\n",
        "\n",
        "minmaxscaler = MinMaxScaler().set_output(transform = 'pandas')\n",
        "\n",
        "train_X9_num = minmaxscaler.fit_transform(train_X9_num)\n",
        "test_X9_num = minmaxscaler.fit_transform(test_X9_num)\n",
        "\n",
        "range_df = train_X9_num.select_dtypes('number').apply(lambda x: x.max() - x.min(), axis = 0)\n",
        "print(range_df)"
      ],
      "metadata": {
        "id": "H8PqTZHuPKe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ìƒì¹˜ ì²˜ë¦¬\n",
        "\n",
        "warpbreaks = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/warpbreaks.csv')\n",
        "warpbreaks.boxplot(column = ['breaks']);\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "Ow_3ddGdPtpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IQRë¥¼ í™œìš©í•œ ì´ìƒì¹˜ ì²˜ë¦¬\n",
        "\n",
        "Q1 = np.quantile(warpbreaks['breaks'], 0.25)\n",
        "Q3 = np.quantile(warpbreaks['breaks'], 0.75)\n",
        "\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "UC = Q3 + ( 1.5 * IQR )\n",
        "LC = Q1 - ( 1.5 * IQR )\n",
        "\n",
        "print(warpbreaks.loc[(warpbreaks.breaks > UC) | (warpbreaks.breaks < LC), :])"
      ],
      "metadata": {
        "id": "dkEOrk3fSTlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Z-scoreë¥¼ ì´ìš©í•œ ë°©ë²• ( í‰ê· ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‘œì¤€í¸ì°¨ ëŒ€ë¹„ ëª‡ë°° ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨.)\n",
        "# Z = (x-í‰ê· )/í‘œì¤€í¸ì°¨. ë³´í†µ Z > 3 ì •ë„ë¡œ ì´ìƒì¹˜ íŒë³„ê¸°ì¤€ì„ ì„¸ì›€.\n",
        "\n",
        "upper = warpbreaks['breaks'].mean() + ( 3 * warpbreaks['breaks'].std())\n",
        "lower = warpbreaks['breaks'].mean() - ( 3 * warpbreaks['breaks'].std())\n",
        "\n",
        "warpbreaks.loc[(warpbreaks.breaks > upper) | (warpbreaks.breaks < lower), :]"
      ],
      "metadata": {
        "id": "wph5iECISZfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ì‚°í™” ë°©ë²•\n",
        "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ê·¸ë£¹í•‘í•˜ì—¬ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ.\n",
        "\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# .T = ì „ì¹˜ì—°ì‚°. (1, 9) ë°°ì—´ì„ (9, 1) ë°°ì—´ë¡œ ë³€ê²½.\n",
        "X = np.array( [[0, 1, 1, 2, 5, 10, 11, 14, 18]] ).T"
      ],
      "metadata": {
        "id": "g0BMrnMOScJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# êµ¬ê°„ ê°„ì˜ ê¸¸ì´ê°€ ë§ë„ë¡.\n",
        "kbd = KBinsDiscretizer(n_bins = 3, strategy = 'uniform')\n",
        "\n",
        "X_bin = kbd.fit_transform(X).toarray()\n",
        "print(kbd.bin_edges_)"
      ],
      "metadata": {
        "id": "zVYeoUYfJ3FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶„ìœ„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ê°„ ë¶„ë¦¬\n",
        "kbd2 = KBinsDiscretizer(n_bins = 3, strategy = 'quantile')\n",
        "\n",
        "X_bin2 = kbd2.fit_transform(X).toarray()\n",
        "print(kbd2.bin_edges_)"
      ],
      "metadata": {
        "id": "dstNIQkmK2fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# êµ¬ê°„ì„ ì„ì˜ë¡œ ì„¤ì •í•˜ëŠ” ë°©ë²•.\n",
        "# pd.cut()ì€ ì—°ì†í˜• ìˆ«ì ë°ì´í„°ë¥¼ êµ¬ê°„(bins) ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ë°ì´í„°ê°€ ì–´ëŠ êµ¬ê°„ì— ì†í•˜ëŠ”ì§€ ë¼ë²¨(labels) ì„ ë¶™ì—¬ì£¼ëŠ” í•¨ìˆ˜ì„.\n",
        "\n",
        "bins = [0, 4, 7, 11, 18]\n",
        "labels = ['A', 'B', 'C', 'D']\n",
        "\n",
        "# X ë°°ì—´ì— ëŒ€í•´ì„œ binsëŠ” êµ¬ê°„ ì„¤ì •. ì°¸ê³ ë¡œ êµ¬ê°„ì€ ì•ì€ ì—´ë¦¼, ë’¤ëŠ” ë‹«í˜ì„. (0, 4] = 0 < x <= 4\n",
        "# í•´ë‹¹ êµ¬ê°„ì— ëŒ€í•œ ë¼ë²¨ì€ ladelsë¡œ ì„¤ì •.\n",
        "\n",
        "# reshape(-1)ì€ 1ì°¨ì› í–‰ë ¬ë¡œ í‰íƒ„í™”í•˜ëŠ” ê²ƒ.\n",
        "X_bin3 = pd.cut(X.reshape(-1), bins = bins, labels = labels)\n",
        "print(X_bin3)"
      ],
      "metadata": {
        "id": "ZeoZ4w6NLHJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ì°¨ì›ì¶•ì†Œ\n",
        "- ë°ì´í„° í¬ì†Œì„± : ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” ê³µê°„ ë‚´ì— ë°ì´í„°ê°€ í¬ì†Œí•´ì§€ë¯€ë¡œ, ë°ì´í„° ê°„ì˜ ê±°ë¦¬ê°€ - ë©€ì–´ì§€ê³  ì˜ë¯¸ìˆëŠ” íŒ¨í„´ì„ ì°¾ê¸° ì–´ë ¤ì›Œì§.\n",
        "- ê³„ì‚° ë³µì¡ì„± ì¦ê°€ : ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡ ê³„ì‚°í•´ì•¼ í•  ì–‘ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€.\n",
        "- ê³¼ì í•© : ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ ê°ì†Œ.\n",
        "\n",
        "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì°¨ì›ì¶•ì†Œë¥¼ ì§„í–‰í•´ì•¼ í•˜ëŠ”ë°, ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” `ì£¼ì„±ë¶„ ë¶„ì„(PCA)`ì´ ìˆìŒ."
      ],
      "metadata": {
        "id": "tDqoUqdMN6Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/df_5.csv')\n",
        "df.columns = ['index', 'X100m', 'Long.jump', 'Shot.put', 'High.jump', 'X400m', 'X110m.hurdle', 'Discus', 'Pole.vault', 'Javeline', 'X1500m']\n",
        "df.set_index('index', inplace = True)\n",
        "\n",
        "print(df.head( ))\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "rOMmD2kAJa3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "3hTh6a-VNVNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = train.corr()\n",
        "sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\", fmt = \".2f\")\n",
        "plt.title(\"Correlation Matrix (Train Data)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AcmFtne7PEc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standardscaler = StandardScaler()\n",
        "\n",
        "train_scaler = standardscaler.fit_transform(train)\n",
        "test_scaler = standardscaler.transform(test)"
      ],
      "metadata": {
        "id": "y7B2_cOuPGkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ì „ì²´ ë³€ìˆ˜ì— ëŒ€í•œ ì£¼ì„±ë¶„ ë¶„ì„ì„ ì§„í–‰í•œ ë’¤ì— np.cumsum(pca.explained_variance_ratio_) ë“±ì„ í™œìš©í•˜ì—¬ ì§ì ‘ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ ì„ íƒí•´ë„ ë˜ê³ ,\n",
        "pca = PCA(n_components=10, svd_solver = 'auto')\n",
        "train_scaler_pca = pca.fit_transform(train_scaler)\n",
        "test_scaler_pca = pca.transform(test_scaler)"
      ],
      "metadata": {
        "id": "FnZXsAiyRB5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cumulative_explained_varience = np.cumsum(pca.explained_variance_ratio_ )\n",
        "\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "# plt.figure(figsize = (8, 6))\n",
        "# plt.plot(range(1, len(cumulative_explained_varience) + 1), cumulative_explained_varience, marker = 'o', linestyle = '-')\n",
        "# plt.xlabel('Number of Principal Components')\n",
        "# plt.ylabel('Cumulative Explained Variance')\n",
        "# plt.title('Scree Plot')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "uV5sfklPRLew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•„ì— ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨ì„ ì„ì˜ë¡œ ì„¤ì •í•˜ì—¬, ì£¼ì„±ë¶„ì˜ ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì •í•˜ê²Œ í•  ìˆ˜ë„ ìˆìŒ.\n",
        "\n",
        "pca2 = PCA(n_components=0.8, svd_solver = 'full')\n",
        "train_scaler_pca2 = pca2.fit_transform(train_scaler)\n",
        "test_scaler_pca2 = pca2.transform(test_scaler)\n",
        "\n",
        "print(pca2.explained_variance_ratio_)\n",
        "print(pca2.n_components_)"
      ],
      "metadata": {
        "id": "KYXW78KcS4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë²”ì£¼í˜• ë³€ìˆ˜ì™€ ì—°ì†í˜• ë³€ìˆ˜ í•œë²ˆì— ì „ì²˜ë¦¬í•˜ê¸°\n",
        "ê¸°ì¡´ì—ëŠ” ë²”ì£¼í˜•ê³¼ ì—°ì†í˜•ì„ ë‚˜ëˆ„ê³ , ê°œë³„ì ìœ¼ë¡œ ì „ì²˜ë¦¬í•œ ë’¤ì— ë³‘í•©í•˜ëŠ” í˜•íƒœë¡œ ì§„í–‰í–ˆë‹¤ë©´, `make_column_transformer()`ë¥¼ ì‚¬ìš©í•˜ë©´, í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ."
      ],
      "metadata": {
        "id": "NWlikF-OUUQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dat = pd.read_csv('https://raw.githubusercontent.com/YoungjinBD/data/main/bda1.csv')\n",
        "\n",
        "y = dat.grade\n",
        "X = dat.drop(['grade'], axis = 1)\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "PjlYRz7pTObn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make_column_transformer í•¨ìˆ˜ í™œìš©í•´ì„œ í•œë²ˆì— ì²˜ë¦¬í•˜ê¸°\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "cat_columns = train_X.select_dtypes('object').columns\n",
        "num_columns = train_X.select_dtypes('number').columns\n",
        "\n",
        "onehotencoder = OneHotEncoder(sparse_output = False, drop = None, handle_unknown='ignore')\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "# ì—¬ê¸°ì—ì„œ remainder = 'passthrough'ëŠ” ì§€ì •í•˜ì§€ ì•ŠëŠ” columnì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ì‘ì—…í• ì§€ì— ëŒ€í•œ íŒŒë¼ë¯¸í„°ì„. default ê°’ì€ dropì´ë¼, ì•„ì— ì‚­ì œì‹œì¼œë²„ë¦¼.\n",
        "mc_transformer = make_column_transformer((stdscaler, num_columns), (onehotencoder, cat_columns), remainder = 'passthrough').set_output(transform = 'pandas')\n",
        "\n",
        "train_X_transformed = mc_transformer.fit_transform(train_X)\n",
        "test_X_transformed = mc_transformer.transform(test_X)\n",
        "\n",
        "print(train_X_transformed.head(2))"
      ],
      "metadata": {
        "id": "oCKD_TMhVFbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ColumnTransformerëŠ” make_column_transformer ì™€ ì™„ì „íˆ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•¨. => ì—°ì†í˜•, ë²”ì£¼í˜• ë°ì´í„° í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "onehotencoder = OneHotEncoder(sparse_output = False, drop = None, handle_unknown='ignore')\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "c_transformer = ColumnTransformer([\n",
        "    ('cat', onehotencoder, cat_columns), ('num', stdscaler, num_columns)\n",
        "]).set_output(transform = 'pandas')\n",
        "\n",
        "train_X2_transformed = c_transformer.fit_transform(train_X)\n",
        "test_X2_transformed = c_transformer.transform(test_X)\n",
        "\n",
        "print(train_X2_transformed.head(2))"
      ],
      "metadata": {
        "id": "xT-oVsNdcPIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ëˆ„ìˆ˜\n",
        "# í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°(ë¯¸ë˜)ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ê²ƒì¸ë°, fit_transform ë“±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê¹Œì§€ë„ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ ë°˜ì˜í•˜ê²Œ ë˜ë©´ ë°ì´í„° ëˆ„ìˆ˜ê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "trainX = pd.DataFrame({'x1': range(1, 9, 1), 'x2': range(15, 23, 1)})\n",
        "testX = pd.DataFrame({'x1': [1, 3, 5], 'x2': [2, 4, 6]})\n",
        "\n",
        "stdscaler = StandardScaler( )\n",
        "trainX2 = stdscaler.fit_transform(trainX)\n",
        "testX2 = stdscaler.transform(testX)\n",
        "\n",
        "print('trainX 1ì˜ í‘œì¤€í™” ë³€í™˜ ê²°ê³¼ :', trainX2[0, 0])\n",
        "print('testX 1ì˜ í‘œì¤€í™” ë³€í™˜ ê²°ê³¼ :', testX2[0, 0])"
      ],
      "metadata": {
        "id": "OyGFD-lJZ_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 03 ì—°ìŠµë¬¸ì œ : scikit-learnì„ í™œìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "8-s2bMHJbT8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u1OVCIIScqTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1-3ë²ˆ"
      ],
      "metadata": {
        "id": "gCkvvR7j0Ev5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 ë‹¤ìŒ ë°ì´í„°ì—ì„œ 'age' ë³€ìˆ˜ì˜ ê²°ì¸¡ì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì¹˜í•œ í›„, 'age' ë³€ìˆ˜ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "\n",
        "data = {\n",
        "'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
        "'age': [25, np.nan, 30, np.nan, 22],\n",
        "'score': [85, 90, 78, 88, 95]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "print(df['age'].mean())"
      ],
      "metadata": {
        "id": "8vZdNrWFbbP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 ë‹¤ìŒ DataFrameì—ì„œ 'city' ë³€ìˆ˜ë¥¼ ì›-í•« ì¸ì½”ë”©(one-hot encoding)í•˜ì—¬ ìƒˆë¡œìš´ DataFrameìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
        "'city': ['Seoul', 'Busan', 'Seoul', 'Daegu', 'Busan']\n",
        "})\n",
        "\n",
        "data_city = data[['city']]\n",
        "\n",
        "onehotencoder = OneHotEncoder(sparse_output = False).set_output(transform = 'pandas')\n",
        "\n",
        "ont_hot_city = onehotencoder.fit_transform(data_city)\n",
        "\n",
        "print(ont_hot_city)"
      ],
      "metadata": {
        "id": "lGG9M0Fybo2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 ë‹¤ìŒ DataFrameì—ì„œ 'height' ë³€ìˆ˜ë¥¼ í‘œì¤€í™”(Standardization)í•œ í›„, í•´ë‹¹ ë³€ìˆ˜ì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì˜ ì°¨ì´ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
        "'height': [165, 170, 175, 180, 185]\n",
        "})\n",
        "\n",
        "data_num = data.select_dtypes('number')\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "data_num = std_scaler.fit_transform(data_num)\n",
        "\n",
        "print(data_num.max() - data_num.min())"
      ],
      "metadata": {
        "id": "5QQrv34HbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### âŒ4ë²ˆ"
      ],
      "metadata": {
        "id": "_HA3wGQxkrl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 ë‹¤ìŒ ë°ì´í„°ì—ì„œ 'city' ë³€ìˆ˜ì˜ ê²°ì¸¡ì¹˜ë¥¼ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì¹˜í•œ í›„, ê° ë„ì‹œë³„ ì¸ì› ìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank'],\n",
        "'city': ['Seoul', np.nan, 'Busan', 'Seoul', np.nan, 'Busan']\n",
        "})\n",
        "\n",
        "simpleimputer = SimpleImputer(strategy = 'most_frequent')\n",
        "\n",
        "data[['city']] = simpleimputer.fit_transform(data[['city']])\n",
        "print(data['city'].value_counts())"
      ],
      "metadata": {
        "id": "ytdp6f2cb9ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5ë²ˆ"
      ],
      "metadata": {
        "id": "99qRNfkL0A39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5 ë‹¤ìŒ ë°ì´í„°ì—ì„œ sales ë³€ìˆ˜ì˜ ì´ìƒì¹˜ë¥¼ ìƒìê·¸ë¦¼ ê¸°ì¤€ìœ¼ë¡œ ì œê±°í•œ í›„, ë‚¨ì€ ë°ì´í„°ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'product': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],\n",
        "'sales': [100, 120, 130, 400, 110, 115, 500]\n",
        "})\n",
        "\n",
        "Q1 = np.quantile(data['sales'], 0.25)\n",
        "Q3 = np.quantile(data['sales'], 0.75)\n",
        "\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "data = data.loc[ ( Q3 + IQR * 1.5 > data['sales'] ) & ( data['sales'] > Q1 - IQR * 1.5 ) ]\n",
        "\n",
        "print(data['sales'].mean())"
      ],
      "metadata": {
        "id": "aGZvRqk5cGDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### âŒ6ë²ˆ"
      ],
      "metadata": {
        "id": "YKaHSggVu8V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6 ë‹¤ìŒ ë°ì´í„°ì—ì„œ score ë³€ìˆ˜ë¥¼ êµ¬ê°„ì˜ í­ì´ ë™ì¼í•˜ë„ë¡ 4ê°œì˜ êµ¬ê°„ìœ¼ë¡œ ì´ì‚°í™”(discretization)í•˜ê³ , ê° êµ¬ê°„ì˜ ë°ì´í„° ê°œìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'name': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],\n",
        "'score': [55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
        "})\n",
        "\n",
        "discretizer = KBinsDiscretizer(n_bins = 4, strategy = 'uniform', encode='ordinal').set_output(transform = 'pandas')\n",
        "data[['score']] = discretizer.fit_transform(data[['score']])\n",
        "\n",
        "print(data['score'].value_counts())"
      ],
      "metadata": {
        "id": "yWvaDc55cLO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### âŒ7ë²ˆ"
      ],
      "metadata": {
        "id": "4dzGB1bTu_-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 ë‹¤ìŒ ë°ì´í„°ì—ì„œ math, english, science 3ê°œ ë³€ìˆ˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì„ ìˆ˜í–‰í•˜ì—¬, ì „ì²´ ë¶„ì‚°ì˜ 80% ì´ìƒì„ ì„¤ëª…í•˜ëŠ” ìµœì†Œ ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ ì¶œë ¥í•˜ì‹œì˜¤.\n",
        "# (ë‹¨, ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒí•˜ê³ , ì£¼ì„±ë¶„ ë¶„ì„ ì „ í‘œì¤€í™”ë¥¼ ì ìš©)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.DataFrame({\n",
        "'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
        "'math': [80, 85, 78, 92, 88],\n",
        "'english': [75, 90, 85, 95, 92],\n",
        "'science': [82, 88, 79, 94, 90]\n",
        "})\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "data_num = data.select_dtypes('number')\n",
        "data_num_standard = std_scaler.fit_transform(data_num)\n",
        "\n",
        "pca = PCA(n_components = 0.8, svd_solver = 'full')\n",
        "\n",
        "data_num_standard_pca = pca.fit_transform(data_num_standard)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.n_components_)\n",
        "print(data_num_standard_pca.shape[1])"
      ],
      "metadata": {
        "id": "Qkhds_W7cT7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}